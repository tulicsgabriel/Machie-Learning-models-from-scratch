{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e909451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "# Sources:\n",
    "# one feature lin regression: https://www.youtube.com/watch?v=p9CsSf_x-wk\n",
    "# in a class: https://www.geeksforgeeks.org/linear-regression-implementation-from-scratch-using-python/\n",
    "# a bit too complicated: https://www.youtube.com/watch?v=RIg3iuen7MY\n",
    "# in a class: https://www.askpython.com/python/examples/linear-regression-from-scratch\n",
    "# Nice solution, but wierd notation: https://github.com/nishant-sg/Machine-Learning-Algorithms/blob/main/Multiple%20Linear%20Regression/Multiple%20Linear%20Regression%20from%20Scratch.ipynb\n",
    "# https://towardsdatascience.com/linear-regression-using-python-b136c91bf0a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd578c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     crim    zn  indus  chas    nox     rm   age     dis  rad  \\\n",
       "0           1  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1   \n",
       "1           2  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2   \n",
       "2           3  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2   \n",
       "3           4  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3   \n",
       "4           5  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3   \n",
       "\n",
       "   tax  ptratio   black  lstat  medv  \n",
       "0  296     15.3  396.90   4.98  24.0  \n",
       "1  242     17.8  396.90   9.14  21.6  \n",
       "2  242     17.8  392.83   4.03  34.7  \n",
       "3  222     18.7  394.63   2.94  33.4  \n",
       "4  222     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Boston.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e02170f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(df[[\"rm\", \"age\", \"dis\"]]) # feature\n",
    "#X = pd.DataFrame(df[[\"rm\"]]) \n",
    "y = pd.DataFrame(df[\"medv\"]) # target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b703bb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6b63bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ceac1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 3)\n",
      "(354, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a802d086",
   "metadata": {},
   "source": [
    "----------\n",
    "## Overview of Linear Regression\n",
    "\n",
    "#### The basic model for multiple linear regression is (I'll call the coefficients weights and the intercept ${\\beta}$)\n",
    "\n",
    "$y_{i}=\\beta _{0}+weight_{1} x_{i}+\\cdots + weight_{n} x_{n} = \\mathbf {X} _{i}^{\\mathsf {T}}{\\boldsymbol {\\beta }} + \\beta _{0},\\qquad i=1,\\ldots ,n$\n",
    "\n",
    "----------\n",
    "\n",
    "#### This that is needed for a linear regression algorithm:\n",
    "- init weights: ${\\beta}_{0}$ = 0, other weights = random\n",
    "\n",
    "----------\n",
    "#### In a loop of iterations:\n",
    "- (forward) Prediction function\n",
    "    - $\\hat{y}$ = np.dot(X, weights) + ${\\beta}_{0}$\n",
    "    \n",
    "    where np.dot stands for matrix multiplication function\n",
    "\n",
    "\n",
    "- Gradient descent, ak. the way to calculate the new weights (it's the result of the chain derivate of the cost function) \n",
    "    - $dW = -\\frac{2}{m}*np.dot(X_T, y-\\hat{y})$\n",
    "    - $d{\\beta}_{0} = -\\frac{2}{m}* \\Sigma_{i=1}^{m}{(y_i-\\hat{y_i})}$\n",
    "\n",
    "\n",
    "- update weights \n",
    "    - $W = W - lr * dW$\n",
    "    - ${\\beta}_{0} = {\\beta}_{0} - lr* d{\\beta}_{0}$ \n",
    "    \n",
    "    where $lr$ is the predefined `learning rate`\n",
    "\n",
    "\n",
    "- loss (in this case MSE) / error / cost calculation\n",
    "    - $\\text{MSE} = \\frac{1}{2m}\\Sigma_{i=1}^{m}{\\Big({y_i - \\hat{y}_i}\\Big)^2}$\n",
    "    \n",
    "    where $m$ is the number of samples -> put it in a list, for plotting it out\n",
    "----------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58825507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(X):\n",
    "    \"\"\" Initializes the weights of the model\n",
    "    \"\"\"\n",
    "    weights = [random.random() for _ in range(X.shape[1])]\n",
    "    weights = np.array(weights)\n",
    "    b0 = 0\n",
    "\n",
    "    print(\"weights initialized!\")\n",
    "    print(b0, weights)\n",
    "    \n",
    "    return b0, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "337b4e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sample, b0, weights):\n",
    "    \"\"\" Predict function\n",
    "    \"\"\"\n",
    "    prediction = round(np.dot(sample, weights) + b0, 3)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94e61b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_model_fit(X, y, learning_rate, iteration):\n",
    "    \"\"\" Linear regression fit function. Calculates weights based on X and y using Gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    m, n = X.shape\n",
    "    b0, weights = init_weights(X)\n",
    "    cost_list = []\n",
    "    \n",
    "    for i in range(0, iteration):\n",
    "        \n",
    "        y_pred = np.dot(X, weights) + b0 # prediction\n",
    "        #y_pred = np.dot(X, weights)\n",
    "        \n",
    "        mse = 1/(2*m) * np.sum(np.square(y - y_pred)) # cost function, OLS, Mean squared error\n",
    "\n",
    "        d_weights = - 2/m * np.dot(X.T, y - y_pred) # Gradient Descent\n",
    "        d_b0 = -2/m * np.sum(y - y_pred)\n",
    "               \n",
    "        weights = weights - learning_rate * d_weights # -> model coefficients\n",
    "        b0 = b0 - learning_rate * d_b0 \n",
    "        cost_list.append(mse)\n",
    "        \n",
    "        # Print error / cost every so often\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Cost: -----------------> {}\".format(np.mean(mse)))\n",
    "        \n",
    "    print(\"\\nLinear Regression model trained.\\n\")\n",
    "    \n",
    "    return b0, weights, cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32c8299f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights initialized!\n",
      "0 [0.0523636  0.08718668 0.40724176]\n",
      "Cost: -----------------> 166.15311695304786\n",
      "Cost: -----------------> 38.5138861805582\n",
      "Cost: -----------------> 33.98852776218473\n",
      "Cost: -----------------> 31.838440232644956\n",
      "Cost: -----------------> 30.14495789681317\n",
      "Cost: -----------------> 28.76481778490595\n",
      "Cost: -----------------> 27.63801398091479\n",
      "Cost: -----------------> 26.71785440719772\n",
      "Cost: -----------------> 25.966326506613058\n",
      "Cost: -----------------> 25.35241332256501\n",
      "Cost: -----------------> 24.85080324388484\n",
      "Cost: -----------------> 24.440840434304285\n",
      "Cost: -----------------> 24.105668452290644\n",
      "Cost: -----------------> 23.83153139722896\n",
      "Cost: -----------------> 23.60720360168965\n",
      "Cost: -----------------> 23.423524225047693\n",
      "Cost: -----------------> 23.273017453085508\n",
      "Cost: -----------------> 23.149582557374224\n",
      "Cost: -----------------> 23.048240964555298\n",
      "Cost: -----------------> 22.964929849227914\n",
      "Cost: -----------------> 22.896333692976455\n",
      "Cost: -----------------> 22.83974682611695\n",
      "Cost: -----------------> 22.79296125325886\n",
      "Cost: -----------------> 22.754175112024264\n",
      "Cost: -----------------> 22.721917969699877\n",
      "Cost: -----------------> 22.69498986068379\n",
      "Cost: -----------------> 22.672411537270907\n",
      "Cost: -----------------> 22.653383871216207\n",
      "Cost: -----------------> 22.637254722898966\n",
      "Cost: -----------------> 22.62349190451121\n",
      "Cost: -----------------> 22.61166111634698\n",
      "Cost: -----------------> 22.601407941449477\n",
      "Cost: -----------------> 22.592443152129302\n",
      "Cost: -----------------> 22.584530719174364\n",
      "Cost: -----------------> 22.577478026623403\n",
      "Cost: -----------------> 22.571127886415905\n",
      "Cost: -----------------> 22.56535202185235\n",
      "Cost: -----------------> 22.56004574969472\n",
      "Cost: -----------------> 22.55512364043153\n",
      "Cost: -----------------> 22.55051597678562\n",
      "Cost: -----------------> 22.54616586363742\n",
      "Cost: -----------------> 22.542026869543545\n",
      "Cost: -----------------> 22.538061102070106\n",
      "Cost: -----------------> 22.534237637145562\n",
      "Cost: -----------------> 22.530531237315547\n",
      "Cost: -----------------> 22.526921305759462\n",
      "Cost: -----------------> 22.52339103270333\n",
      "Cost: -----------------> 22.519926698839893\n",
      "Cost: -----------------> 22.51651710687626\n",
      "Cost: -----------------> 22.513153117641693\n",
      "Cost: -----------------> 22.509827271522802\n",
      "Cost: -----------------> 22.506533479531274\n",
      "Cost: -----------------> 22.503266771195992\n",
      "Cost: -----------------> 22.500023088827465\n",
      "Cost: -----------------> 22.4967991196248\n",
      "Cost: -----------------> 22.493592158664637\n",
      "Cost: -----------------> 22.490399997091668\n",
      "Cost: -----------------> 22.48722083087511\n",
      "Cost: -----------------> 22.484053186348415\n",
      "Cost: -----------------> 22.480895859444995\n",
      "Cost: -----------------> 22.477747866110874\n",
      "Cost: -----------------> 22.474608401838307\n",
      "Cost: -----------------> 22.471476808642667\n",
      "Cost: -----------------> 22.468352548113646\n",
      "Cost: -----------------> 22.46523517942325\n",
      "Cost: -----------------> 22.462124341379027\n",
      "Cost: -----------------> 22.459019737778352\n",
      "Cost: -----------------> 22.45592112545668\n",
      "Cost: -----------------> 22.452828304534048\n",
      "Cost: -----------------> 22.449741110455758\n",
      "Cost: -----------------> 22.446659407496984\n",
      "Cost: -----------------> 22.443583083462162\n",
      "Cost: -----------------> 22.440512045359338\n",
      "Cost: -----------------> 22.437446215870224\n",
      "Cost: -----------------> 22.434385530469434\n",
      "Cost: -----------------> 22.431329935073723\n",
      "Cost: -----------------> 22.428279384123513\n",
      "Cost: -----------------> 22.42523383901737\n",
      "Cost: -----------------> 22.422193266834373\n",
      "Cost: -----------------> 22.41915763929157\n",
      "Cost: -----------------> 22.416126931893075\n",
      "Cost: -----------------> 22.413101123235798\n",
      "Cost: -----------------> 22.410080194442784\n",
      "Cost: -----------------> 22.40706412870083\n",
      "Cost: -----------------> 22.404052910883127\n",
      "Cost: -----------------> 22.401046527241302\n",
      "Cost: -----------------> 22.39804496515413\n",
      "Cost: -----------------> 22.39504821292244\n",
      "Cost: -----------------> 22.392056259601727\n",
      "Cost: -----------------> 22.389069094865622\n",
      "Cost: -----------------> 22.386086708894357\n",
      "Cost: -----------------> 22.38310909228386\n",
      "Cost: -----------------> 22.38013623597147\n",
      "Cost: -----------------> 22.377168131175395\n",
      "Cost: -----------------> 22.374204769345234\n",
      "Cost: -----------------> 22.37124614212162\n",
      "Cost: -----------------> 22.368292241303315\n",
      "Cost: -----------------> 22.365343058820315\n",
      "Cost: -----------------> 22.362398586711898\n",
      "Cost: -----------------> 22.359458817108735\n",
      "\n",
      "Linear Regression model trained.\n",
      "\n",
      "-1.1648286830792416 [ 6.30068808 -0.16299647 -1.13948156]\n"
     ]
    }
   ],
   "source": [
    "iteration = 100000\n",
    "learning_rate = 0.00005\n",
    "b0, weights, cost_list = linear_regression_model_fit(X_train, y_train.values.flatten(), learning_rate = learning_rate, iteration = iteration)\n",
    "\n",
    "print(b0, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67452d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2 = X_test.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aff0d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: [6.416, 84.1, 2.6463] -> prediction: 22.537\n",
      "Sample: [6.758, 32.9, 4.0776] -> prediction: 31.406\n",
      "Sample: [5.983, 98.8, 1.8681] -> prediction: 18.299\n",
      "Sample: [6.065, 7.8, 5.2873] -> prediction: 29.753\n",
      "Sample: [6.297, 91.8, 2.3682] -> prediction: 20.849\n",
      "Sample: [6.279, 74.5, 4.0522] -> prediction: 21.637\n",
      "Sample: [5.914, 83.2, 3.9986] -> prediction: 17.98\n",
      "Sample: [6.174, 93.6, 1.6119] -> prediction: 20.642\n",
      "Sample: [6.229, 90.7, 3.0993] -> prediction: 19.767\n",
      "Sample: [6.027, 79.7, 2.4982] -> prediction: 20.972\n",
      "Sample: [5.951, 93.8, 2.8893] -> prediction: 17.749\n",
      "Sample: [6.004, 85.9, 6.5921] -> prediction: 15.152\n",
      "Sample: [4.519, 100.0, 1.6582] -> prediction: 9.119\n",
      "Sample: [6.232, 53.7, 5.0141] -> prediction: 23.635\n",
      "Sample: [5.708, 74.3, 4.7211] -> prediction: 17.309\n",
      "Sample: [6.98, 67.6, 2.5329] -> prediction: 28.909\n",
      "Sample: [5.879, 95.8, 2.0063] -> prediction: 17.976\n",
      "Sample: [4.88, 100.0, 1.5895] -> prediction: 11.472\n",
      "Sample: [7.875, 32.0, 5.6484] -> prediction: 36.801\n",
      "Sample: [6.185, 98.7, 2.2616] -> prediction: 19.14\n",
      "Sample: [6.24, 16.3, 4.429] -> prediction: 30.448\n",
      "Sample: [6.482, 32.1, 4.1403] -> prediction: 29.726\n",
      "Sample: [5.713, 94.1, 4.233] -> prediction: 14.67\n",
      "Sample: [6.593, 69.1, 2.4786] -> prediction: 26.288\n",
      "Sample: [6.006, 95.3, 1.8746] -> prediction: 19.007\n",
      "Sample: [5.427, 95.4, 2.4298] -> prediction: 14.71\n",
      "Sample: [5.874, 36.6, 4.5026] -> prediction: 24.749\n",
      "Sample: [6.728, 94.1, 2.4961] -> prediction: 23.044\n",
      "Sample: [5.96, 92.1, 3.8771] -> prediction: 16.957\n",
      "Sample: [5.794, 70.6, 2.8927] -> prediction: 20.538\n",
      "Sample: [5.572, 88.5, 2.5961] -> prediction: 16.559\n",
      "Sample: [6.426, 52.3, 4.5404] -> prediction: 25.625\n",
      "Sample: [7.313, 97.9, 1.3163] -> prediction: 27.455\n",
      "Sample: [5.362, 96.2, 2.1036] -> prediction: 14.542\n",
      "Sample: [6.003, 94.5, 2.5403] -> prediction: 18.36\n",
      "Sample: [5.709, 98.5, 1.6232] -> prediction: 16.901\n",
      "Sample: [7.185, 61.1, 4.9671] -> prediction: 28.487\n",
      "Sample: [5.869, 46.3, 5.2311] -> prediction: 22.306\n",
      "Sample: [6.064, 59.1, 4.2392] -> prediction: 22.579\n",
      "Sample: [6.245, 6.2, 5.2873] -> prediction: 31.148\n",
      "Sample: [5.39, 72.9, 2.7986] -> prediction: 17.724\n",
      "Sample: [6.849, 70.3, 3.1827] -> prediction: 26.903\n",
      "Sample: [8.034, 31.9, 5.118] -> prediction: 38.423\n",
      "Sample: [5.594, 36.8, 6.498] -> prediction: 20.679\n",
      "Sample: [6.417, 66.1, 3.0923] -> prediction: 24.969\n",
      "Sample: [5.713, 97.0, 1.9265] -> prediction: 16.825\n",
      "Sample: [5.701, 95.0, 3.7872] -> prediction: 14.955\n",
      "Sample: [6.417, 6.6, 5.2873] -> prediction: 32.166\n",
      "Sample: [6.167, 84.0, 3.0334] -> prediction: 20.543\n",
      "Sample: [6.575, 65.2, 4.09] -> prediction: 24.974\n",
      "Sample: [6.009, 82.9, 6.2267] -> prediction: 16.088\n",
      "Sample: [6.968, 37.2, 5.2447] -> prediction: 30.699\n",
      "Sample: [6.142, 91.7, 3.9769] -> prediction: 18.056\n",
      "Sample: [6.781, 71.3, 2.8561] -> prediction: 26.684\n",
      "Sample: [7.47, 52.6, 2.872] -> prediction: 34.055\n",
      "Sample: [6.312, 51.9, 3.9917] -> prediction: 25.597\n",
      "Sample: [6.219, 100.0, 2.0048] -> prediction: 19.435\n",
      "Sample: [6.861, 27.9, 5.1167] -> prediction: 31.686\n",
      "Sample: [6.389, 48.0, 4.7794] -> prediction: 25.82\n",
      "Sample: [5.605, 70.2, 7.9549] -> prediction: 13.644\n",
      "Sample: [6.762, 43.4, 7.9809] -> prediction: 25.272\n",
      "Sample: [7.249, 21.9, 8.6966] -> prediction: 31.03\n",
      "Sample: [6.552, 21.4, 3.3751] -> prediction: 32.783\n",
      "Sample: [5.456, 36.6, 3.7965] -> prediction: 22.92\n",
      "Sample: [6.579, 35.9, 10.7103] -> prediction: 22.232\n",
      "Sample: [5.344, 100.0, 3.875] -> prediction: 11.791\n",
      "Sample: [6.471, 98.8, 1.7257] -> prediction: 21.536\n",
      "Sample: [6.302, 32.2, 5.4007] -> prediction: 27.14\n",
      "Sample: [6.595, 21.8, 5.4011] -> prediction: 30.68\n",
      "Sample: [5.976, 87.9, 2.5806] -> prediction: 19.22\n",
      "Sample: [5.786, 33.3, 5.1004] -> prediction: 24.051\n",
      "Sample: [6.75, 74.9, 3.3317] -> prediction: 25.36\n",
      "Sample: [5.854, 96.6, 1.8956] -> prediction: 17.814\n",
      "Sample: [5.936, 19.5, 10.5857] -> prediction: 20.995\n",
      "Sample: [6.14, 45.8, 4.0905] -> prediction: 25.395\n",
      "Sample: [5.453, 100.0, 1.4896] -> prediction: 15.196\n",
      "Sample: [6.167, 90.0, 2.421] -> prediction: 20.263\n",
      "Sample: [7.853, 33.2, 5.118] -> prediction: 37.071\n",
      "Sample: [6.545, 99.1, 1.5192] -> prediction: 22.189\n",
      "Sample: [5.414, 98.3, 1.7554] -> prediction: 14.924\n",
      "Sample: [5.885, 33.0, 6.498] -> prediction: 23.131\n",
      "Sample: [5.617, 97.9, 1.4547] -> prediction: 16.611\n",
      "Sample: [5.876, 19.1, 9.2203] -> prediction: 22.238\n",
      "Sample: [5.036, 97.0, 1.77] -> prediction: 12.738\n",
      "Sample: [6.023, 90.4, 2.834] -> prediction: 18.82\n",
      "Sample: [6.726, 66.5, 3.6519] -> prediction: 26.213\n",
      "Sample: [6.701, 90.0, 2.5975] -> prediction: 23.427\n",
      "Sample: [6.415, 40.1, 4.7211] -> prediction: 27.338\n",
      "Sample: [6.211, 28.9, 3.6659] -> prediction: 29.081\n",
      "Sample: [6.454, 98.4, 1.8498] -> prediction: 21.353\n",
      "Sample: [6.02, 47.2, 3.5549] -> prediction: 25.021\n",
      "Sample: [5.304, 89.1, 1.6475] -> prediction: 15.854\n",
      "Sample: [6.326, 97.7, 2.271] -> prediction: 20.181\n",
      "Sample: [5.898, 52.3, 8.0136] -> prediction: 18.341\n",
      "Sample: [3.561, 87.9, 1.6132] -> prediction: 5.106\n",
      "Sample: [6.372, 97.9, 2.3274] -> prediction: 20.374\n",
      "Sample: [6.216, 100.0, 1.1691] -> prediction: 20.368\n",
      "Sample: [4.628, 100.0, 1.5539] -> prediction: 9.924\n",
      "Sample: [6.461, 93.3, 2.0026] -> prediction: 22.054\n",
      "Sample: [6.657, 100.0, 1.5275] -> prediction: 22.739\n",
      "Sample: [6.015, 45.1, 4.4272] -> prediction: 24.338\n",
      "Sample: [6.286, 45.0, 4.5026] -> prediction: 25.976\n",
      "Sample: [5.304, 97.3, 2.1007] -> prediction: 14.001\n",
      "Sample: [5.834, 56.5, 4.4986] -> prediction: 21.258\n",
      "Sample: [6.031, 23.3, 6.6407] -> prediction: 25.47\n",
      "Sample: [6.103, 85.1, 2.0218] -> prediction: 21.113\n",
      "Sample: [5.727, 69.5, 3.7965] -> prediction: 19.265\n",
      "Sample: [6.431, 14.7, 5.4159] -> prediction: 30.788\n",
      "Sample: [6.037, 34.5, 5.9853] -> prediction: 24.429\n",
      "Sample: [6.383, 35.7, 9.1876] -> prediction: 22.764\n",
      "Sample: [6.152, 100.0, 1.9142] -> prediction: 19.116\n",
      "Sample: [6.852, 100.0, 1.4655] -> prediction: 24.038\n",
      "Sample: [6.31, 38.5, 6.4584] -> prediction: 24.958\n",
      "Sample: [6.144, 62.2, 2.5979] -> prediction: 24.448\n",
      "Sample: [7.163, 79.9, 3.2157] -> prediction: 27.279\n",
      "Sample: [6.208, 95.0, 2.2222] -> prediction: 19.933\n",
      "Sample: [7.765, 83.3, 2.741] -> prediction: 31.059\n",
      "Sample: [5.599, 85.7, 4.4546] -> prediction: 15.068\n",
      "Sample: [5.707, 54.0, 2.3817] -> prediction: 23.277\n",
      "Sample: [6.481, 18.5, 6.1899] -> prediction: 29.601\n",
      "Sample: [5.593, 76.5, 7.9549] -> prediction: 12.541\n",
      "Sample: [6.487, 13.0, 7.3967] -> prediction: 29.16\n",
      "Sample: [5.957, 100.0, 1.8026] -> prediction: 18.015\n",
      "Sample: [6.021, 82.6, 2.7474] -> prediction: 20.177\n",
      "Sample: [6.169, 6.6, 5.7209] -> prediction: 30.109\n",
      "Sample: [6.041, 49.9, 4.7211] -> prediction: 23.384\n",
      "Sample: [6.516, 27.7, 8.5353] -> prediction: 25.65\n",
      "Sample: [7.155, 92.2, 2.7006] -> prediction: 25.811\n",
      "Sample: [6.152, 82.6, 1.7455] -> prediction: 22.145\n",
      "Sample: [7.82, 64.5, 4.6947] -> prediction: 32.244\n",
      "Sample: [5.613, 95.6, 1.7572] -> prediction: 16.616\n",
      "Sample: [6.333, 17.2, 5.2146] -> prediction: 29.992\n",
      "Sample: [6.072, 100.0, 4.175] -> prediction: 16.036\n",
      "Sample: [6.092, 95.4, 2.548] -> prediction: 18.766\n",
      "Sample: [5.186, 93.8, 1.5296] -> prediction: 14.479\n",
      "Sample: [6.425, 74.8, 2.2004] -> prediction: 24.618\n",
      "Sample: [6.059, 37.3, 4.8122] -> prediction: 25.448\n",
      "Sample: [6.816, 40.5, 8.3248] -> prediction: 25.693\n",
      "Sample: [6.604, 18.8, 6.2196] -> prediction: 30.293\n",
      "Sample: [5.924, 94.1, 4.3996] -> prediction: 15.809\n",
      "Sample: [5.99, 81.7, 4.2579] -> prediction: 18.408\n",
      "Sample: [6.345, 20.1, 7.8278] -> prediction: 26.617\n",
      "Sample: [5.787, 31.1, 6.6115] -> prediction: 22.694\n",
      "Sample: [5.404, 88.6, 3.665] -> prediction: 14.266\n",
      "Sample: [5.531, 85.4, 1.6074] -> prediction: 17.933\n",
      "Sample: [6.249, 77.3, 3.615] -> prediction: 21.489\n",
      "Sample: [6.129, 96.0, 1.7494] -> prediction: 19.811\n",
      "Sample: [6.406, 97.2, 2.0651] -> prediction: 21.001\n",
      "Sample: [5.813, 100.0, 4.0952] -> prediction: 14.495\n",
      "Sample: [8.725, 83.0, 2.8944] -> prediction: 36.982\n",
      "Sample: [6.436, 87.9, 2.3158] -> prediction: 22.42\n",
      "Sample: [6.251, 96.6, 2.198] -> prediction: 19.971\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "\n",
    "for sample in X_test2:\n",
    "    mod_pred = predict(sample, b0, weights)\n",
    "    y_pred.append(mod_pred)\n",
    "    \n",
    "    print(\"Sample: {} -> prediction: {}\".format(sample, mod_pred))\n",
    "\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8855553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2 = y_test.values.tolist()\n",
    "y_test2_flatten = [num[0] for num in y_test2]\n",
    "# y_test2_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bd056c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the cost function\n",
    "plt.plot(np.arange(iteration), cost_list)\n",
    "plt.xlabel(\"iter\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e3db238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom implementation intercept & coefs: \n",
      "-1.1648286830792416 [ 6.30068808 -0.16299647 -1.13948156]\n",
      "\n",
      "\n",
      "sklearn intercept & coefs: \n",
      "[-22.46897352] [[ 8.59574587 -0.09950527 -0.54254192]]\n"
     ]
    }
   ],
   "source": [
    "# Compare it to sklearn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "clf = LinearRegression(fit_intercept=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Custom implementation intercept & coefs: \")\n",
    "print(b0, weights)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"sklearn intercept & coefs: \")\n",
    "print(clf.intercept_, clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84667867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custom_weights</th>\n",
       "      <th>sklearn_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.300688</td>\n",
       "      <td>8.595746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.162996</td>\n",
       "      <td>-0.099505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.139482</td>\n",
       "      <td>-0.542542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   custom_weights  sklearn_weights\n",
       "0        6.300688         8.595746\n",
       "1       -0.162996        -0.099505\n",
       "2       -1.139482        -0.542542"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\"custom_weights\": weights, \"sklearn_weights\": clf.coef_[0]}\n",
    "\n",
    "df_weights = pd.DataFrame(d)\n",
    "df_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea310490",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sk = clf.predict(X_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c74ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b977661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom implementation RMSE & Pearson coeff:\n",
      "RMSE: 6.136913959586562\n",
      "Pearson: 0.7116048533982424\n",
      "\n",
      "sklearn implementation RMSE & Pearson coeff:\n",
      "RMSE: 5.921424190324507\n",
      "Pearson: 0.7302123709247088\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "r, p = pearsonr(y_test2_flatten, y_pred)\n",
    "\n",
    "print(\"Custom implementation RMSE & Pearson coeff:\")\n",
    "print(\"RMSE: {}\".format(rmse))\n",
    "print(\"Pearson: {}\".format(r))\n",
    "\n",
    "\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred_sk))\n",
    "r, p = pearsonr(y_test2_flatten, y_pred_sk)\n",
    "\n",
    "print(\"\\nsklearn implementation RMSE & Pearson coeff:\")\n",
    "print(\"RMSE: {}\".format(rmse))\n",
    "print(\"Pearson: {}\".format(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a763df75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
