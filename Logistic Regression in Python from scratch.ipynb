{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f705fdc4",
   "metadata": {},
   "source": [
    "## Logistic Regression in Python from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "577213b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# disable chained assignments\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "###\n",
    "# Sources:\n",
    "# https://beckernick.github.io/logistic-regression-from-scratch/\n",
    "# https://www.askpython.com/python/examples/logistic-regression-from-scratch\n",
    "# https://towardsdatascience.com/logistic-regression-from-scratch-in-python-ec66603592e2\n",
    "# https://github.com/beckernick/logistic_regression_from_scratch/blob/master/logistic_regression_scratch.ipynb\n",
    "# https://dhirajkumarblog.medium.com/logistic-regression-in-python-from-scratch-5b901d72d68e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbf18d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings/Spouses Aboard</th>\n",
       "      <th>Parents/Children Aboard</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. Owen Harris Braund</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. John Bradley (Florence Briggs Thayer) Cum...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Miss. Laina Heikkinen</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. Jacques Heath (Lily May Peel) Futrelle</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. William Henry Allen</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass                                               Name  \\\n",
       "0         0       3                             Mr. Owen Harris Braund   \n",
       "1         1       1  Mrs. John Bradley (Florence Briggs Thayer) Cum...   \n",
       "2         1       3                              Miss. Laina Heikkinen   \n",
       "3         1       1        Mrs. Jacques Heath (Lily May Peel) Futrelle   \n",
       "4         0       3                            Mr. William Henry Allen   \n",
       "\n",
       "      Sex   Age  Siblings/Spouses Aboard  Parents/Children Aboard     Fare  \n",
       "0    male  22.0                        1                        0   7.2500  \n",
       "1  female  38.0                        1                        0  71.2833  \n",
       "2  female  26.0                        0                        0   7.9250  \n",
       "3  female  35.0                        1                        0  53.1000  \n",
       "4    male  35.0                        0                        0   8.0500  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"titanic.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c8ad17b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'Siblings/Spouses Aboard',\n",
       "       'Parents/Children Aboard', 'Fare'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "239a426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Survived\"]\n",
    "X = df[['Pclass', 'Sex', 'Age', 'Siblings/Spouses Aboard','Parents/Children Aboard', 'Fare']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb3f5527",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"Sex\"] = np.where(X[\"Sex\"] == \"male\", 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8757d558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings/Spouses Aboard</th>\n",
       "      <th>Parents/Children Aboard</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Sex   Age  Siblings/Spouses Aboard  Parents/Children Aboard  \\\n",
       "0       3    0  22.0                        1                        0   \n",
       "1       1    1  38.0                        1                        0   \n",
       "2       3    1  26.0                        0                        0   \n",
       "3       1    1  35.0                        1                        0   \n",
       "4       3    0  35.0                        0                        0   \n",
       "\n",
       "      Fare  \n",
       "0   7.2500  \n",
       "1  71.2833  \n",
       "2   7.9250  \n",
       "3  53.1000  \n",
       "4   8.0500  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "462054ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6815e2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train :  (620, 6)\n",
      "Shape of Y_train :  (620,)\n",
      "Shape of X_test :  (267, 6)\n",
      "Shape of Y_test :  (267,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train : \", X_train.shape)\n",
    "print(\"Shape of Y_train : \", y_train.shape)\n",
    "print(\"Shape of X_test : \", X_test.shape)\n",
    "print(\"Shape of Y_test : \", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fe8973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.values\n",
    "# y_train = y_train.values\n",
    "# X_test = X_test.values\n",
    "# y_test = y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1868f257",
   "metadata": {},
   "source": [
    "\n",
    "### This that is needed for a Logistic Regression algorithm:\n",
    "\n",
    "- init weights: ${\\beta}_{0}$ = 0, other weights = random\n",
    "\n",
    "----------\n",
    "#### In a loop of iterations:\n",
    "- (forward) Prediction function\n",
    "    - $\\hat{y}$ = np.dot(X, weights) + ${\\beta}_{0}$\n",
    "    - Apply Sigmoid function -> $\\hat{y} = sigmoid(\\hat{y})$\n",
    "    \n",
    "    np.dot stands for matrix multiplication function\n",
    "    \n",
    " \n",
    "- Gradient descent, ak. the way to calculate the new weights ($dW$ and $d{\\beta}_{0}$ are the gradient equations \n",
    "    of the log likelihood loss function, thus they are different than in the case of linear regression)\n",
    "\n",
    "    - $dW = \\frac {1}{m} * np.dot(X_T, (\\hat{y} - y))$\n",
    "    - $d{\\beta}_{0} = \\frac {1}{m} * \\Sigma_{i=1}^{m}{(\\hat{y} - y)}$\n",
    "\n",
    "\n",
    "- update weights \n",
    "    - $W = W - lr * dW$\n",
    "    - ${\\beta}_{0} = {\\beta}_{0} - lr * d{\\beta}_{0}$ \n",
    "    \n",
    "    where $lr$ is the predefined `learning rate`\n",
    "\n",
    "\n",
    "- Instead of Mean Squared Error, we use a cost function called Cross-Entropy, also known as Log Loss. loss / error / cost calculation:\n",
    "    - $loss = -\\frac{1}{m} (y * \\log(\\hat{y}) - (1-y) * \\log(1-\\hat{y}))$\n",
    "    \n",
    "    put it in a list, for plotting it out\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5b4fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(X):\n",
    "    \"\"\" Initializes the weights of the model\n",
    "    \"\"\"\n",
    "    weights = [random.random() for _ in range(X.shape[1])]\n",
    "    weights = np.array(weights)\n",
    "    b0 = 0\n",
    "\n",
    "    print(\"weights initialized!\\n\")\n",
    "    print(b0, weights)\n",
    "    \n",
    "    return b0, weights\n",
    "\n",
    "\n",
    "def predict(sample, b0, weights):\n",
    "    \"\"\" Prediction function\n",
    "    \"\"\"\n",
    "    prediction = round(np.dot(sample, weights) + b0, 3)\n",
    "    prediction = sigmoid(prediction) # without this it gives linear prediction\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "def sigmoid(scores):\n",
    "    \"\"\" Sigmoid function\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdcd62bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_model_fit(X, y, iteration, learning_rate):\n",
    "    \"\"\" Logistic regression fit function. Calculates weights based on X and y using Gradient descent\n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    b0, weights = init_weights(X)\n",
    "    cost_list = []\n",
    "    \n",
    "    for i in range(0, iteration):\n",
    "        y_pred = np.dot(X, weights) + b0 # prediction\n",
    "        y_pred = sigmoid(y_pred)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = -np.mean(y*(np.log(y_pred)) - (1-y)*np.log(1-y_pred))\n",
    "        \n",
    "        # Calculate the gradients\n",
    "        d_weights = (1/m)*np.dot(X.T, (y_pred - y))\n",
    "        d_b0 = (1/m)*np.sum((y_pred - y)) \n",
    "          \n",
    "        # Update weights with gradient\n",
    "        weights = weights - learning_rate * d_weights\n",
    "        b0 = b0 - learning_rate * d_b0 \n",
    "        \n",
    "        cost_list.append(loss)\n",
    "        \n",
    "        # Print error / cost every so often\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Cost: -----------------> {}\".format(loss))\n",
    "    \n",
    "    print(\"\\nLogistic Regression model trained.\\n\")\n",
    "    \n",
    "    return b0, weights, cost_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53904785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights initialized!\n",
      "\n",
      "0 [0.0523636  0.08718668 0.40724176 0.10770023 0.90119888 0.03815367]\n",
      "Cost: -----------------> -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40971/3735444261.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.mean(y*(np.log(y_pred)) - (1-y)*np.log(1-y_pred))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: -----------------> -0.2951951627033406\n",
      "Cost: -----------------> -0.23734562991834657\n",
      "Cost: -----------------> -0.20324229633328092\n",
      "Cost: -----------------> -0.174433590639749\n",
      "Cost: -----------------> -0.15004211466092374\n",
      "Cost: -----------------> -0.12970177604666863\n",
      "Cost: -----------------> -0.11288153901246065\n",
      "Cost: -----------------> -0.09901018629404514\n",
      "Cost: -----------------> -0.08756448342021816\n",
      "Cost: -----------------> -0.07810125085319787\n",
      "Cost: -----------------> -0.07025936646969586\n",
      "Cost: -----------------> -0.06374955978752848\n",
      "Cost: -----------------> -0.05834109789448641\n",
      "Cost: -----------------> -0.05384926723595355\n",
      "Cost: -----------------> -0.05012496138973276\n",
      "Cost: -----------------> -0.04704653343281878\n",
      "Cost: -----------------> -0.04451363001024555\n",
      "Cost: -----------------> -0.04244260531834681\n",
      "Cost: -----------------> -0.04076312891004207\n",
      "Cost: -----------------> -0.03941566479657749\n",
      "Cost: -----------------> -0.038349572275868535\n",
      "Cost: -----------------> -0.03752164517436989\n",
      "Cost: -----------------> -0.03689496009726767\n",
      "Cost: -----------------> -0.03643794510976106\n",
      "Cost: -----------------> -0.036123609522907635\n",
      "Cost: -----------------> -0.035928895457244064\n",
      "Cost: -----------------> -0.03583412499469424\n",
      "Cost: -----------------> -0.035822525077043296\n",
      "Cost: -----------------> -0.03587981748747628\n",
      "Cost: -----------------> -0.035993864430987774\n",
      "Cost: -----------------> -0.036154362202626515\n",
      "Cost: -----------------> -0.03635257670648911\n",
      "Cost: -----------------> -0.03658111547049296\n",
      "Cost: -----------------> -0.036833731468678736\n",
      "Cost: -----------------> -0.03710515460975881\n",
      "Cost: -----------------> -0.03739094722645886\n",
      "Cost: -----------------> -0.03768738032758379\n",
      "Cost: -----------------> -0.03799132776327146\n",
      "Cost: -----------------> -0.03830017580720787\n",
      "Cost: -----------------> -0.03861174597908648\n",
      "Cost: -----------------> -0.038924229217278714\n",
      "Cost: -----------------> -0.03923612976676709\n",
      "Cost: -----------------> -0.03954621737265247\n",
      "Cost: -----------------> -0.0398534865670667\n",
      "Cost: -----------------> -0.04015712200950969\n",
      "Cost: -----------------> -0.04045646898999877\n",
      "Cost: -----------------> -0.04075100833343284\n",
      "Cost: -----------------> -0.04104033505464937\n",
      "Cost: -----------------> -0.04132414020904246\n",
      "Cost: -----------------> -0.04160219546529087\n",
      "Cost: -----------------> -0.04187433999663328\n",
      "Cost: -----------------> -0.042140469346819386\n",
      "Cost: -----------------> -0.04240052597775917\n",
      "Cost: -----------------> -0.04265449124932832\n",
      "Cost: -----------------> -0.04290237861876487\n",
      "Cost: -----------------> -0.04314422787859592\n",
      "Cost: -----------------> -0.04338010027887245\n",
      "Cost: -----------------> -0.04361007440232221\n",
      "Cost: -----------------> -0.043834242680477205\n",
      "Cost: -----------------> -0.044052708455407\n",
      "Cost: -----------------> -0.044265583505763845\n",
      "Cost: -----------------> -0.04447298596787476\n",
      "Cost: -----------------> -0.044675038592831756\n",
      "Cost: -----------------> -0.04487186728924489\n",
      "Cost: -----------------> -0.04506359990875308\n",
      "Cost: -----------------> -0.04525036523770637\n",
      "Cost: -----------------> -0.045432292163830296\n",
      "Cost: -----------------> -0.045609508991287274\n",
      "Cost: -----------------> -0.0457821428814698\n",
      "Cost: -----------------> -0.045950319400185215\n",
      "Cost: -----------------> -0.04611416215478777\n",
      "Cost: -----------------> -0.046273792507201046\n",
      "Cost: -----------------> -0.046429329350890954\n",
      "Cost: -----------------> -0.04658088894159817\n",
      "Cost: -----------------> -0.04672858477316019\n",
      "Cost: -----------------> -0.04687252749105324\n",
      "Cost: -----------------> -0.047012824837369994\n",
      "Cost: -----------------> -0.04714958162190777\n",
      "Cost: -----------------> -0.04728289971484114\n",
      "Cost: -----------------> -0.047412878057126226\n",
      "Cost: -----------------> -0.047539612685385126\n",
      "Cost: -----------------> -0.04766319676851773\n",
      "Cost: -----------------> -0.047783720653700394\n",
      "Cost: -----------------> -0.04790127191981469\n",
      "Cost: -----------------> -0.04801593543663296\n",
      "Cost: -----------------> -0.04812779342838994\n",
      "Cost: -----------------> -0.04823692554054429\n",
      "Cost: -----------------> -0.04834340890877937\n",
      "Cost: -----------------> -0.04844731822941623\n",
      "Cost: -----------------> -0.048548725830557894\n",
      "Cost: -----------------> -0.04864770174342823\n",
      "Cost: -----------------> -0.04874431377342642\n",
      "Cost: -----------------> -0.04883862757053564\n",
      "Cost: -----------------> -0.048930706698784625\n",
      "Cost: -----------------> -0.04902061270451614\n",
      "Cost: -----------------> -0.049108405183280336\n",
      "Cost: -----------------> -0.04919414184520903\n",
      "Cost: -----------------> -0.04927787857873801\n",
      "Cost: -----------------> -0.04935966951263532\n",
      "\n",
      "Logistic Regression model trained.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iteration = 100000\n",
    "learning_rate = 0.005\n",
    "\n",
    "b0, weights, cost_list = logistic_regression_model_fit(X_train, y_train.values.flatten(), learning_rate = learning_rate, iteration = iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be8ad24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(620, 6)\n",
      "(6,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "321fa41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0010337665772653 [-1.05876709  3.07831146 -0.0370515  -0.55905229 -0.17472939  0.01567855]\n"
     ]
    }
   ],
   "source": [
    "print(b0, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a07884fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEGCAYAAABlxeIAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAARLUlEQVR4nO3de3BmdX3H8fcnyWZZl9uubLnIJaBUB5EiBgp4KVaKSLFbLCpKRbR2rZe2tlYHpGq149SKVetoha21rQ5VWwviAIqitHSsosFyU0ARUWFEgkyFeuXy7R/PL2xYN9ksu09Okuf9mnkm5/zOyXO+J78n+eTcU1VIkjTUdQGSpIXBQJAkAQaCJKkxECRJgIEgSWpGui5gS+yyyy41NjbWdRmStKhcccUVd1TVms3Nt6gCYWxsjImJia7LkKRFJcm35zKfu4wkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQMSCN+988f8xw23d12GJC1oi+rCtIfqmHdexk/uuY+b3/qbXZciSQvWQGwh/OSe+7ouQZIWvIEIhCljp13I2GkXct/9PiVOkjY2UIEw5ZGvu4jP33hH12VI0oIykIEAcPL7L2fstAuZuPnOrkuRpAVhIA4qz+bEs74AwHbLhrjyDcew3bLhjiuSpG4MfCBM+ek99/OY13/qgfG/O/kQnnHgbiTpsCpJmj8Gwgxefs5XfqHtpU/Zj3VP2Y+Hb7+8g4okqb8MhC1w9mU3cfZlN806z+H7reb4g/bg0LHV7LV6BSuWDbuVIWlRMBC2sS/edCdfvGl+DlSvXjnK6pWjrHrYMnZasYwdt1vGjiuWsXL5MA8bHWHl6DArRofZbtkwo8NDLF82xLLhqVcYGRpieCiMDIeRoTCUMNy+Dg2FocBQQgKhN56EwANtK0aHGR0Z2HMTpL677/7ip/fcx/KRIUaG+/u7ZiAsYnf+6Ofc+aOfd12GpHlwyhH78Oa1B/Z1Gf5rJ0mLwAe/MKfHIm+VTgMhybFJbkhyY5LTuqxFkgZdZ4GQZBh4L/AM4ADgeUkO6KoeSRp0XW4hHAbcWFU3VdXPgY8AazusR5IGWpeB8Ajgu9PGb2ltkqQOLPiDyknWJZlIMjE5Odl1OZK0ZHUZCLcCe00b37O1PUhVra+q8aoaX7NmzbwVJ0kLyRuf2f9DrF1eh/BlYP8k+9ILgpOA53dYT2dWjg7z5P3XMD62isfstiN7rlrB6u1HWTk6wvCQVzlLmh+dBUJV3ZvklcDFwDDwgar6alf1bEvPOuQRvOjIfXnM7juwrM9XFkrSttLplcpVdRFwUZc1PFQvO+qRvOKpj2L75V7sLWlp8K/ZHJx06F68ae1jWT7isxIkLV0Gwibssv1y/vM1R7HS//4lDRD/4jWH77eac15yuAdxJQ2sgQ+EiT8/ml184I0kDW4gfPwVT+TgvXbuugxJWjAG8pzIt5xwoGEgSRsZyEA4+Vf36boESVpwBi4QTj1yrOsSJGlBGrhAmI/7gUjSYjRwgZB4WqkkbcrABYIkadMGKhCue/OxXZcgSQvWQATCqUeOscN2I6wY9V5EkjSTgQgEAI8cSNLsBiYQJEmzMxAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJQEeBkOTZSb6a5P4k413UIEl6sK62EK4FngVc1tHyJUkbGelioVV1HfiwGklaSBb8MYQk65JMJJmYnJzsuhxJWrL6toWQ5BJgt01MOqOqzp/r+1TVemA9wPj4eG2j8iRJG+lbIFTV0f16b0nStrfgdxlJkuZHV6ednpDkFuAI4MIkF3dRhyRpg67OMjoPOK+LZUuSNs1dRpIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIGKBB8frMkzW5gAkGSNDsDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSmk4CIcmZSa5PcnWS85Ls3EUdkqQNutpC+AxwYFUdBHwdOL2jOiRJTSeBUFWfrqp72+gXgT27qEOStMFCOIbwYuCTM01Msi7JRJKJycnJeSxLkgbLSL/eOMklwG6bmHRGVZ3f5jkDuBc4Z6b3qar1wHqA8fHx6kOpkiT6GAhVdfRs05OcChwPPK2q/EMvSR3rWyDMJsmxwGuBX6uqH3dRgyTpwbo6hvAeYAfgM0muTHJWR3VIkppOthCq6lFdLFeSNLOFcJaRJGkBMBAkSYCBIElqDARJEmAgSJKaOQVCkg/NpU2StHjNdQvhsdNHkgwDT9j25UiSujJrICQ5PcndwEFJ7mqvu4HbgfPnpUJJ0ryYNRCq6q+qagfgzKrasb12qKqHV5XPMJCkJWSuu4wuSLISIMnvJnlHkn36WJckaZ7NNRDeB/w4ya8Arwa+CXywb1VJkubdXAPh3naL6rXAe6rqvfRuTidJWiLmenO7u5OcDrwAeHKSIWBZ/8qSJM23uW4hPBf4GfDiqrqN3jOQz+xbVduYz9+RpM2bUyC0EDgH2CnJ8cBPq2pRHUNIuq5Akha2uV6p/BzgS8CzgecAlyc5sZ+FSZLm11yPIZwBHFpVtwMkWQNcAnysX4VJkubXXI8hDE2FQfODLfheSdIiMNcthE8luRj4cBt/LnBRf0qSJHVh1kBI8ihg16p6TZJnAU9qk75A7yCzJGmJ2NwWwruA0wGq6lzgXIAkj2vTntnH2iRJ82hzxwF2raprNm5sbWN9qUiS1InNBcLOs0xbsQ3rkCR1bHOBMJHk9zduTPIS4IqHutAkf5nk6iRXJvl0kj0e6ntJkraNzR1DeBVwXpKT2RAA48AocMJWLPfMqno9QJI/At4A/MFWvJ8kaSvNGghV9X3gyCRPBQ5szRdW1ee2ZqFVdde00ZWANxuSpI7N6TqEqroUuHRbLjjJW4BTgB8CT51lvnXAOoC99957W5YgSZqmb1cbJ7kkybWbeK0FqKozqmovetczvHKm96mq9VU1XlXja9as6Ve5kjTw5nql8harqqPnOOs59K56fmO/apEkbV4n9yNKsv+00bXA9V3UIUnaoG9bCJvx1iSPBu4Hvo1nGElS5zoJhKr6nS6WK0mambewliQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJKagQiE6roASVoEBiIQANJ1AZK0wA1MIEiSZmcgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJTaeBkOTVSSrJLl3WIUnqMBCS7AUcA3ynqxokSRt0uYXwTuC1eGcJSVoQOgmEJGuBW6vqqjnMuy7JRJKJycnJeahOkgbTSL/eOMklwG6bmHQG8Dp6u4s2q6rWA+sBxsfH3ZqQpD7pWyBU1dGbak/yOGBf4KokAHsCX0lyWFXd1q96JEmz61sgzKSqrgF+aWo8yc3AeFXdMd+1SJI28DoESRLQwRbCxqpqrOsaJEluIUiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoGIhDKJzFL0mYNRCAAtOc3S5JmMDCBIEmanYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1nQRCkr9IcmuSK9vruC7qkCRtMNLhst9ZVW/vcPmSpGncZSRJAroNhFcmuTrJB5KsmmmmJOuSTCSZmJycnM/6JGmg9C0QklyS5NpNvNYC7wMeCRwMfA/4m5nep6rWV9V4VY2vWbOmX+VK0sDr2zGEqjp6LvMl+Xvggn7VIUmam67OMtp92ugJwLVd1CFJ2qCrs4zeluRgoICbgZd2VIckqekkEKrqBV0sV5I0M087lSQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBHT3gJx59dg9duRn997XdRmStKANRCCcdNjenHTY3l2XIUkLmruMJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpSVV1XcOcJZkEvv0Qv30X4I5tWM5i4DoPBtd5MGzNOu9TVWs2N9OiCoStkWSiqsa7rmM+uc6DwXUeDPOxzu4ykiQBBoIkqRmkQFjfdQEdcJ0Hg+s8GPq+zgNzDEGSNLtB2kKQJM3CQJAkAQMSCEmOTXJDkhuTnNZ1PVsiyV5JLk3ytSRfTfLHrX11ks8k+Ub7uqq1J8m727peneSQae/1wjb/N5K8cFr7E5Jc077n3Uky/2v6i5IMJ/mfJBe08X2TXN7q/GiS0da+vI3f2KaPTXuP01v7DUmePq19wX0mkuyc5GNJrk9yXZIjlno/J/mT9rm+NsmHk2y31Po5yQeS3J7k2mltfe/XmZYxq6pa0i9gGPgmsB8wClwFHNB1XVtQ/+7AIW14B+DrwAHA24DTWvtpwF+34eOATwIBDgcub+2rgZva11VteFWb9qU2b9r3PqPr9W51/SnwL8AFbfxfgZPa8FnAy9rwy4Gz2vBJwEfb8AGtv5cD+7bPwfBC/UwA/wy8pA2PAjsv5X4GHgF8C1gxrX9PXWr9DDwFOAS4dlpb3/t1pmXMWmvXvwTz0BlHABdPGz8dOL3rurZifc4HfgO4Adi9te0O3NCGzwaeN23+G9r05wFnT2s/u7XtDlw/rf1B83W4nnsCnwV+HbigfdjvAEY27lfgYuCINjzS5svGfT0130L8TAA7tT+O2ah9yfYzvUD4bvsjN9L6+elLsZ+BMR4cCH3v15mWMdtrEHYZTX3optzS2hadton8eOByYNeq+l6bdBuwaxueaX1na79lE+1dexfwWuD+Nv5w4H+r6t42Pr3OB9atTf9hm39LfxZd2heYBP6x7SZ7f5KVLOF+rqpbgbcD3wG+R6/frmBp9/OU+ejXmZYxo0EIhCUhyfbAvwOvqqq7pk+r3r8AS+b84STHA7dX1RVd1zKPRujtVnhfVT0e+BG9zfwHLMF+XgWspReGewArgWM7LaoD89Gvc13GIATCrcBe08b3bG2LRpJl9MLgnKo6tzV/P8nubfruwO2tfab1na19z020d+mJwG8luRn4CL3dRn8L7JxkpM0zvc4H1q1N3wn4AVv+s+jSLcAtVXV5G/8YvYBYyv18NPCtqpqsqnuAc+n1/VLu5ynz0a8zLWNGgxAIXwb2b2cujNI7GPWJjmuas3bGwD8A11XVO6ZN+gQwdabBC+kdW5hqP6WdrXA48MO22XgxcEySVe0/s2Po7V/9HnBXksPbsk6Z9l6dqKrTq2rPqhqj11+fq6qTgUuBE9tsG6/z1M/ixDZ/tfaT2tkp+wL70zsAt+A+E1V1G/DdJI9uTU8DvsYS7md6u4oOT/KwVtPUOi/Zfp5mPvp1pmXMrMuDSvN4QOc4emfnfBM4o+t6trD2J9Hb1LsauLK9jqO37/SzwDeAS4DVbf4A723reg0wPu29Xgzc2F4vmtY+Dlzbvuc9bHRgs+P1P4oNZxntR+8X/Ubg34DlrX27Nn5jm77ftO8/o63XDUw7q2YhfiaAg4GJ1tcfp3c2yZLuZ+BNwPWtrg/RO1NoSfUz8GF6x0juobcl+Hvz0a8zLWO2l7eukCQBg7HLSJI0BwaCJAkwECRJjYEgSQIMBElSYyBIc5Tkv9vXsSTP77oeaVszEKQ5qqoj2+AYsEWBMO3KW2nBMhCkOUryf23wrcCTk1yZ3v38h5OcmeTL7R72L23zH5Xkv5J8gt4VuNKC5n8t0pY7DfizqjoeIMk6ercYODTJcuDzST7d5j0EOLCqvtVRrdKcGQjS1jsGOCjJ1P13dqJ3P52fA18yDLRYGAjS1gvwh1V18YMak6Po3cZaWhQ8hiBtubvpPc50ysXAy9ptyknyy+3hNtKi4haCtOWuBu5LchXwT/Se1TAGfKXdgngS+O2uipMeKu92KkkC3GUkSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqfl/hwraPDXfJg8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(iteration), cost_list)\n",
    "plt.xlabel(\"iter\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e161ca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2 = X_test.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b52b0edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: [1.0, 0.0, 47.0, 0.0, 0.0, 30.5] -> prediction: 0.42043205953893\n",
      "Sample: [2.0, 0.0, 25.0, 1.0, 2.0, 41.5792] -> prediction: 0.21433336342519133\n",
      "Sample: [3.0, 0.0, 69.0, 0.0, 0.0, 14.5] -> prediction: 0.02919863268418474\n",
      "Sample: [1.0, 0.0, 56.0, 0.0, 0.0, 35.5] -> prediction: 0.3598535120310333\n",
      "Sample: [2.0, 0.0, 57.0, 0.0, 0.0, 12.35] -> prediction: 0.11557684467553356\n",
      "Sample: [3.0, 1.0, 14.0, 1.0, 0.0, 11.2417] -> prediction: 0.7314516207339741\n",
      "Sample: [3.0, 0.0, 17.0, 1.0, 1.0, 7.2292] -> prediction: 0.08121143222282061\n",
      "Sample: [3.0, 0.0, 22.0, 0.0, 0.0, 8.05] -> prediction: 0.13423750027220732\n",
      "Sample: [3.0, 1.0, 28.0, 0.0, 0.0, 7.7375] -> prediction: 0.7284949591003717\n",
      "Sample: [3.0, 1.0, 24.0, 1.0, 0.0, 15.85] -> prediction: 0.6688525748543953\n",
      "Sample: [1.0, 0.0, 34.0, 0.0, 0.0, 26.55] -> prediction: 0.524729805230163\n",
      "Sample: [2.0, 1.0, 40.0, 1.0, 1.0, 39.0] -> prediction: 0.7952716864761252\n",
      "Sample: [3.0, 1.0, 29.0, 0.0, 2.0, 15.2458] -> prediction: 0.6721664514898772\n",
      "Sample: [3.0, 0.0, 49.0, 0.0, 0.0, 0.0] -> prediction: 0.04783412323016055\n",
      "Sample: [3.0, 1.0, 31.0, 1.0, 1.0, 20.525] -> prediction: 0.5846762648954498\n",
      "Sample: [2.0, 0.0, 39.0, 0.0, 0.0, 13.0] -> prediction: 0.20456554696662882\n",
      "Sample: [3.0, 0.0, 51.0, 0.0, 0.0, 7.75] -> prediction: 0.050068395787654964\n",
      "Sample: [1.0, 1.0, 42.0, 1.0, 0.0, 133.65] -> prediction: 0.9820314442330851\n",
      "Sample: [2.0, 0.0, 23.0, 2.0, 1.0, 11.5] -> prediction: 0.11095728256327266\n",
      "Sample: [3.0, 0.0, 19.0, 0.0, 0.0, 10.1708] -> prediction: 0.15187116365665929\n",
      "Sample: [3.0, 0.0, 28.0, 1.0, 0.0, 24.15] -> prediction: 0.08370804132569445\n",
      "Sample: [3.0, 0.0, 4.0, 1.0, 1.0, 11.1333] -> prediction: 0.1320446672233248\n",
      "Sample: [3.0, 1.0, 24.0, 0.0, 0.0, 8.85] -> prediction: 0.760058452516041\n",
      "Sample: [3.0, 1.0, 17.0, 4.0, 2.0, 7.925] -> prediction: 0.23361682498897077\n",
      "Sample: [3.0, 1.0, 16.0, 5.0, 2.0, 46.9] -> prediction: 0.24992731117175176\n",
      "Sample: [1.0, 0.0, 40.0, 0.0, 0.0, 27.7208] -> prediction: 0.4737740906279313\n",
      "Sample: [2.0, 0.0, 47.0, 0.0, 0.0, 15.0] -> prediction: 0.1647915489712316\n",
      "Sample: [2.0, 0.0, 37.0, 1.0, 0.0, 26.0] -> prediction: 0.16260117867931323\n",
      "Sample: [3.0, 0.0, 27.0, 1.0, 0.0, 14.4542] -> prediction: 0.07529924979530686\n",
      "Sample: [1.0, 0.0, 28.0, 0.0, 0.0, 35.5] -> prediction: 0.6132513771084882\n",
      "Sample: [3.0, 1.0, 14.5, 1.0, 0.0, 14.4542] -> prediction: 0.7374970948865424\n",
      "Sample: [1.0, 0.0, 58.0, 0.0, 2.0, 113.275] -> prediction: 0.5547792351072148\n",
      "Sample: [2.0, 1.0, 27.0, 1.0, 0.0, 13.8583] -> prediction: 0.8347951298093854\n",
      "Sample: [3.0, 1.0, 27.0, 0.0, 0.0, 7.925] -> prediction: 0.7363338700215354\n",
      "Sample: [1.0, 0.0, 26.0, 0.0, 0.0, 30.0] -> prediction: 0.6104014487007485\n",
      "Sample: [2.0, 0.0, 59.0, 0.0, 0.0, 13.5] -> prediction: 0.10997465594900203\n",
      "Sample: [3.0, 0.0, 32.0, 0.0, 0.0, 7.8958] -> prediction: 0.09647673224256427\n",
      "Sample: [1.0, 0.0, 35.0, 0.0, 0.0, 26.55] -> prediction: 0.5154950367412013\n",
      "Sample: [2.0, 1.0, 24.0, 0.0, 0.0, 13.0] -> prediction: 0.9068697682797762\n",
      "Sample: [3.0, 1.0, 2.0, 0.0, 1.0, 10.4625] -> prediction: 0.8603259697972047\n",
      "Sample: [3.0, 0.0, 3.0, 1.0, 1.0, 15.9] -> prediction: 0.14541801325254833\n",
      "Sample: [3.0, 0.0, 14.0, 5.0, 2.0, 46.9] -> prediction: 0.016254458934931416\n",
      "Sample: [2.0, 1.0, 30.0, 0.0, 0.0, 13.0] -> prediction: 0.8863511729214218\n",
      "Sample: [3.0, 0.0, 20.0, 0.0, 0.0, 8.05] -> prediction: 0.14307272348008082\n",
      "Sample: [2.0, 0.0, 27.0, 0.0, 0.0, 13.0] -> prediction: 0.28638633694283533\n",
      "Sample: [3.0, 0.0, 44.0, 0.0, 0.0, 7.925] -> prediction: 0.06410385569599905\n",
      "Sample: [3.0, 0.0, 27.0, 1.0, 0.0, 15.5] -> prediction: 0.07642091619189675\n",
      "Sample: [3.0, 0.0, 25.0, 0.0, 0.0, 7.8958] -> prediction: 0.12153220056839735\n",
      "Sample: [3.0, 0.0, 26.0, 1.0, 0.0, 14.4542] -> prediction: 0.07791635939744972\n",
      "Sample: [3.0, 0.0, 30.0, 0.0, 0.0, 8.05] -> prediction: 0.10330777942156368\n",
      "Sample: [3.0, 0.0, 9.0, 5.0, 2.0, 46.9] -> prediction: 0.019493274427854115\n",
      "Sample: [3.0, 0.0, 24.0, 2.0, 0.0, 23.25] -> prediction: 0.056359204935028884\n",
      "Sample: [3.0, 0.0, 18.0, 0.0, 0.0, 7.7958] -> prediction: 0.15187116365665929\n",
      "Sample: [3.0, 0.0, 36.0, 0.0, 0.0, 7.4958] -> prediction: 0.08378477426774988\n",
      "Sample: [2.0, 0.0, 3.0, 1.0, 1.0, 26.0] -> prediction: 0.3649368773130494\n",
      "Sample: [2.0, 0.0, 36.0, 0.0, 0.0, 12.875] -> prediction: 0.2230465403980427\n",
      "Sample: [1.0, 0.0, 45.0, 0.0, 0.0, 30.0] -> prediction: 0.4365932137378064\n",
      "Sample: [3.0, 0.0, 32.0, 0.0, 0.0, 56.4958] -> prediction: 0.18618382790150853\n",
      "Sample: [1.0, 1.0, 25.0, 1.0, 2.0, 151.55] -> prediction: 0.9896667354883095\n",
      "Sample: [3.0, 0.0, 26.0, 0.0, 0.0, 56.4958] -> prediction: 0.2221812565114738\n",
      "Sample: [1.0, 0.0, 64.0, 0.0, 0.0, 26.0] -> prediction: 0.26483265843126774\n",
      "Sample: [3.0, 1.0, 23.0, 0.0, 0.0, 7.8792] -> prediction: 0.7638672733326819\n",
      "Sample: [3.0, 0.0, 32.0, 0.0, 0.0, 7.925] -> prediction: 0.09647673224256427\n",
      "Sample: [2.0, 1.0, 34.0, 0.0, 0.0, 10.5] -> prediction: 0.86611077069459\n",
      "Sample: [1.0, 0.0, 27.0, 1.0, 0.0, 53.1] -> prediction: 0.5535439031511183\n",
      "Sample: [3.0, 0.0, 21.0, 0.0, 0.0, 16.1] -> prediction: 0.15433470446067873\n",
      "Sample: [3.0, 0.0, 21.0, 0.0, 0.0, 8.6625] -> prediction: 0.13967403020279542\n",
      "Sample: [2.0, 1.0, 31.0, 1.0, 1.0, 26.25] -> prediction: 0.8162283196669704\n",
      "Sample: [3.0, 0.0, 30.0, 0.0, 0.0, 7.8958] -> prediction: 0.1031226557932129\n",
      "Sample: [1.0, 0.0, 39.0, 0.0, 0.0, 0.0] -> prediction: 0.37683591710036907\n",
      "Sample: [3.0, 0.0, 19.0, 0.0, 0.0, 8.05] -> prediction: 0.14766920974173398\n",
      "Sample: [1.0, 1.0, 16.0, 0.0, 1.0, 57.9792] -> prediction: 0.9846625281985116\n",
      "Sample: [1.0, 0.0, 45.0, 0.0, 0.0, 26.55] -> prediction: 0.4233588462724936\n",
      "Sample: [3.0, 0.0, 26.0, 0.0, 0.0, 7.8958] -> prediction: 0.11763699212507706\n",
      "Sample: [3.0, 0.0, 19.0, 0.0, 0.0, 8.1583] -> prediction: 0.14792111319292217\n",
      "Sample: [2.0, 0.0, 0.83, 1.0, 1.0, 18.75] -> prediction: 0.35732348927287944\n",
      "Sample: [1.0, 1.0, 48.0, 0.0, 1.0, 55.0] -> prediction: 0.9493096750687493\n",
      "Sample: [3.0, 1.0, 30.0, 0.0, 0.0, 8.6625] -> prediction: 0.7164662543716478\n",
      "Sample: [2.0, 1.0, 57.0, 0.0, 0.0, 10.5] -> prediction: 0.7339975165215696\n",
      "Sample: [3.0, 0.0, 22.0, 0.0, 0.0, 7.25] -> prediction: 0.1327338399028711\n",
      "Sample: [2.0, 0.0, 30.0, 0.0, 0.0, 10.5] -> prediction: 0.25673656833163033\n",
      "Sample: [3.0, 0.0, 30.0, 0.0, 0.0, 7.75] -> prediction: 0.10293782581717216\n",
      "Sample: [1.0, 0.0, 60.0, 0.0, 0.0, 26.55] -> prediction: 0.29629576200022933\n",
      "Sample: [3.0, 0.0, 28.0, 0.0, 0.0, 8.05] -> prediction: 0.11046501233951578\n",
      "Sample: [1.0, 1.0, 18.0, 0.0, 2.0, 79.65] -> prediction: 0.9859779098969845\n",
      "Sample: [1.0, 1.0, 22.0, 0.0, 2.0, 49.5] -> prediction: 0.9742190417471894\n",
      "Sample: [1.0, 0.0, 27.0, 0.0, 2.0, 211.5] -> prediction: 0.9482404915637136\n",
      "Sample: [3.0, 0.0, 28.0, 0.0, 0.0, 8.05] -> prediction: 0.11046501233951578\n",
      "Sample: [3.0, 0.0, 19.0, 0.0, 0.0, 8.05] -> prediction: 0.14766920974173398\n",
      "Sample: [3.0, 0.0, 22.0, 1.0, 0.0, 7.75] -> prediction: 0.08106232489041715\n",
      "Sample: [3.0, 0.0, 0.42, 0.0, 1.0, 8.5167] -> prediction: 0.22583157066402545\n",
      "Sample: [1.0, 0.0, 50.0, 1.0, 0.0, 55.9] -> prediction: 0.35594681144072693\n",
      "Sample: [3.0, 0.0, 19.0, 0.0, 0.0, 7.8958] -> prediction: 0.14741766105380757\n",
      "Sample: [2.0, 0.0, 2.0, 1.0, 1.0, 26.0] -> prediction: 0.37355400118155185\n",
      "Sample: [3.0, 0.0, 16.0, 2.0, 0.0, 18.0] -> prediction: 0.06888143039767268\n",
      "Sample: [3.0, 1.0, 19.0, 1.0, 0.0, 14.4542] -> prediction: 0.7041210772038413\n",
      "Sample: [3.0, 0.0, 30.0, 0.0, 0.0, 7.25] -> prediction: 0.10220143584673717\n",
      "Sample: [3.0, 0.0, 40.0, 0.0, 0.0, 7.8958] -> prediction: 0.07350876054159503\n",
      "Sample: [1.0, 1.0, 17.0, 1.0, 0.0, 108.9] -> prediction: 0.9894288576876098\n",
      "Sample: [2.0, 1.0, 22.0, 1.0, 1.0, 29.0] -> prediction: 0.8662266911487845\n",
      "Sample: [1.0, 0.0, 52.0, 1.0, 1.0, 79.65] -> prediction: 0.38485295749078957\n",
      "Sample: [2.0, 0.0, 42.0, 0.0, 0.0, 13.0] -> prediction: 0.18709465662650418\n",
      "Sample: [3.0, 0.0, 18.0, 0.0, 0.0, 8.3] -> prediction: 0.15290448648785823\n",
      "Sample: [1.0, 1.0, 48.0, 1.0, 0.0, 52.0] -> prediction: 0.9240014935558328\n",
      "Sample: [3.0, 0.0, 38.0, 0.0, 0.0, 7.05] -> prediction: 0.07777278984135669\n",
      "Sample: [3.0, 0.0, 20.0, 0.0, 0.0, 9.225] -> prediction: 0.1452937856970798\n",
      "Sample: [3.0, 1.0, 40.0, 0.0, 2.0, 15.2458] -> prediction: 0.577129331002952\n",
      "Sample: [1.0, 0.0, 44.0, 2.0, 0.0, 90.0] -> prediction: 0.40251423417298393\n",
      "Sample: [3.0, 0.0, 20.0, 0.0, 0.0, 9.8458] -> prediction: 0.14654003083602316\n",
      "Sample: [1.0, 0.0, 23.0, 0.0, 1.0, 63.3583] -> prediction: 0.7127954882429666\n",
      "Sample: [2.0, 1.0, 35.0, 0.0, 0.0, 21.0] -> prediction: 0.8801656757429104\n",
      "Sample: [3.0, 0.0, 27.0, 0.0, 0.0, 7.8958] -> prediction: 0.11385044834696255\n",
      "Sample: [3.0, 0.0, 39.0, 0.0, 0.0, 7.25] -> prediction: 0.07536890864644387\n",
      "Sample: [3.0, 1.0, 29.0, 1.0, 1.0, 10.4625] -> prediction: 0.5643904537836758\n",
      "Sample: [3.0, 0.0, 17.0, 0.0, 0.0, 7.75] -> prediction: 0.15656652916384997\n",
      "Sample: [3.0, 0.0, 25.0, 1.0, 0.0, 7.775] -> prediction: 0.0731689597464982\n",
      "Sample: [3.0, 1.0, 30.0, 0.0, 0.0, 12.475] -> prediction: 0.7284949591003717\n",
      "Sample: [3.0, 1.0, 16.0, 0.0, 0.0, 7.8292] -> prediction: 0.8073683201908555\n",
      "Sample: [3.0, 0.0, 44.0, 0.0, 0.0, 8.05] -> prediction: 0.0642239494555431\n",
      "Sample: [3.0, 0.0, 16.0, 0.0, 0.0, 9.2167] -> prediction: 0.1646539598093883\n",
      "Sample: [1.0, 0.0, 37.0, 1.0, 0.0, 53.1] -> prediction: 0.46132739479349205\n",
      "Sample: [3.0, 0.0, 29.0, 0.0, 0.0, 7.75] -> prediction: 0.1064050079974321\n",
      "Sample: [1.0, 0.0, 43.0, 0.0, 0.0, 30.6958] -> prediction: 0.45760205922564895\n",
      "Sample: [1.0, 0.0, 64.0, 0.0, 0.0, 27.7208] -> prediction: 0.27012272713050856\n",
      "Sample: [3.0, 1.0, 18.0, 1.0, 0.0, 14.4583] -> prediction: 0.7117708097419858\n",
      "Sample: [3.0, 0.0, 19.0, 1.0, 0.0, 19.9667] -> prediction: 0.10669059394565118\n",
      "Sample: [1.0, 0.0, 40.0, 0.0, 0.0, 31.0] -> prediction: 0.4865032795436883\n",
      "Sample: [2.0, 1.0, 38.0, 0.0, 0.0, 13.0] -> prediction: 0.8529589973371989\n",
      "Sample: [2.0, 1.0, 29.0, 0.0, 0.0, 10.5] -> prediction: 0.8861495516530375\n",
      "Sample: [1.0, 1.0, 18.0, 1.0, 0.0, 227.525] -> prediction: 0.9982770622031829\n",
      "Sample: [2.0, 0.0, 28.0, 0.0, 1.0, 33.0] -> prediction: 0.30767744270711384\n",
      "Sample: [1.0, 0.0, 49.0, 1.0, 0.0, 56.9292] -> prediction: 0.3681875822638983\n",
      "Sample: [3.0, 0.0, 28.0, 0.0, 0.0, 7.8958] -> prediction: 0.1101705691679347\n",
      "Sample: [2.0, 0.0, 21.0, 0.0, 0.0, 0.0] -> prediction: 0.2900790816655468\n",
      "Sample: [2.0, 0.0, 18.0, 0.0, 0.0, 13.0] -> prediction: 0.35893259366518293\n",
      "Sample: [3.0, 0.0, 19.0, 0.0, 0.0, 0.0] -> prediction: 0.13250377783228992\n",
      "Sample: [3.0, 0.0, 7.0, 4.0, 1.0, 39.6875] -> prediction: 0.038309307174638485\n",
      "Sample: [3.0, 0.0, 21.0, 0.0, 0.0, 8.4333] -> prediction: 0.1393139241530906\n",
      "Sample: [3.0, 0.0, 32.0, 0.0, 0.0, 7.8958] -> prediction: 0.09647673224256427\n",
      "Sample: [3.0, 0.0, 27.0, 0.0, 0.0, 7.8958] -> prediction: 0.11385044834696255\n",
      "Sample: [2.0, 1.0, 44.0, 1.0, 0.0, 26.0] -> prediction: 0.7649478037637647\n",
      "Sample: [3.0, 1.0, 39.0, 0.0, 5.0, 29.125] -> prediction: 0.5102485643871594\n",
      "Sample: [3.0, 0.0, 28.0, 0.0, 0.0, 56.4958] -> prediction: 0.20965603336987848\n",
      "Sample: [3.0, 0.0, 24.0, 0.0, 0.0, 7.05] -> prediction: 0.12411785352653222\n",
      "Sample: [2.0, 0.0, 66.0, 0.0, 0.0, 10.5] -> prediction: 0.083401747743734\n",
      "Sample: [1.0, 1.0, 48.0, 1.0, 0.0, 146.5208] -> prediction: 0.9816571093022567\n",
      "Sample: [3.0, 0.0, 43.0, 0.0, 0.0, 8.05] -> prediction: 0.0664838004911327\n",
      "Sample: [2.0, 0.0, 23.0, 0.0, 0.0, 10.5] -> prediction: 0.309170530929853\n",
      "Sample: [1.0, 0.0, 47.0, 0.0, 0.0, 34.0208] -> prediction: 0.4338893515030434\n",
      "Sample: [3.0, 0.0, 24.0, 0.0, 0.0, 7.8958] -> prediction: 0.1255380371629388\n",
      "Sample: [1.0, 0.0, 47.0, 0.0, 0.0, 52.0] -> prediction: 0.5039999146688512\n",
      "Sample: [2.0, 0.0, 0.83, 0.0, 2.0, 29.0] -> prediction: 0.4895015432277752\n",
      "Sample: [3.0, 0.0, 18.0, 1.0, 1.0, 7.8542] -> prediction: 0.07921944159466343\n",
      "Sample: [3.0, 0.0, 42.0, 0.0, 1.0, 8.4042] -> prediction: 0.058745445017041875\n",
      "Sample: [1.0, 0.0, 28.0, 0.0, 0.0, 47.1] -> prediction: 0.655431299503486\n",
      "Sample: [2.0, 1.0, 24.0, 1.0, 0.0, 26.0] -> prediction: 0.8722499051749796\n",
      "Sample: [2.0, 0.0, 22.0, 0.0, 0.0, 0.0] -> prediction: 0.28251913408433676\n",
      "Sample: [1.0, 1.0, 53.0, 2.0, 0.0, 51.4792] -> prediction: 0.8514475744762745\n",
      "Sample: [3.0, 1.0, 35.0, 0.0, 0.0, 7.2292] -> prediction: 0.6726070170677604\n",
      "Sample: [2.0, 0.0, 26.0, 0.0, 0.0, 10.5] -> prediction: 0.28597777322269835\n",
      "Sample: [3.0, 1.0, 8.0, 3.0, 1.0, 25.4667] -> prediction: 0.5384240911869264\n",
      "Sample: [3.0, 0.0, 27.0, 0.0, 0.0, 8.4583] -> prediction: 0.11476160549680758\n",
      "Sample: [1.0, 0.0, 46.0, 0.0, 0.0, 35.5] -> prediction: 0.4486813516217848\n",
      "Sample: [1.0, 0.0, 37.0, 1.0, 1.0, 52.5542] -> prediction: 0.4162953826316075\n",
      "Sample: [1.0, 0.0, 65.0, 0.0, 0.0, 26.55] -> prediction: 0.259225100817846\n",
      "Sample: [1.0, 1.0, 31.0, 0.0, 2.0, 164.8667] -> prediction: 0.9939822146247904\n",
      "Sample: [2.0, 0.0, 29.0, 1.0, 0.0, 27.7208] -> prediction: 0.211651365991901\n",
      "Sample: [2.0, 1.0, 34.0, 0.0, 1.0, 23.0] -> prediction: 0.8685273236150377\n",
      "Sample: [2.0, 0.0, 52.0, 0.0, 0.0, 13.5] -> prediction: 0.13800018860561808\n",
      "Sample: [1.0, 0.0, 36.0, 0.0, 0.0, 26.3875] -> prediction: 0.5054997781774029\n",
      "Sample: [1.0, 1.0, 16.0, 0.0, 1.0, 39.4] -> prediction: 0.9795878178035051\n",
      "Sample: [2.0, 1.0, 42.0, 1.0, 0.0, 26.0] -> prediction: 0.777991512221503\n",
      "Sample: [3.0, 0.0, 18.0, 0.0, 0.0, 7.225] -> prediction: 0.15071553540504964\n",
      "Sample: [3.0, 1.0, 38.0, 1.0, 5.0, 31.3875] -> prediction: 0.3903122216951099\n",
      "Sample: [3.0, 1.0, 33.0, 3.0, 0.0, 15.85] -> prediction: 0.3212572468913136\n",
      "Sample: [3.0, 1.0, 4.0, 1.0, 1.0, 16.7] -> prediction: 0.7829600100656982\n",
      "Sample: [1.0, 0.0, 4.0, 0.0, 2.0, 81.8583] -> prediction: 0.8491564195186723\n",
      "Sample: [1.0, 0.0, 46.0, 0.0, 0.0, 39.6] -> prediction: 0.46480828456469886\n",
      "Sample: [2.0, 0.0, 23.0, 0.0, 0.0, 13.0] -> prediction: 0.3175616978069352\n",
      "Sample: [3.0, 0.0, 28.0, 2.0, 0.0, 7.925] -> prediction: 0.038903148508885\n",
      "Sample: [3.0, 1.0, 2.0, 4.0, 2.0, 31.275] -> prediction: 0.4338893515030434\n",
      "Sample: [2.0, 1.0, 28.0, 0.0, 0.0, 13.0] -> prediction: 0.8935949920025679\n",
      "Sample: [1.0, 1.0, 18.0, 2.0, 2.0, 262.375] -> prediction: 0.9975273768433653\n",
      "Sample: [3.0, 0.0, 23.0, 0.0, 0.0, 7.8958] -> prediction: 0.12965642276890066\n",
      "Sample: [2.0, 1.0, 21.0, 0.0, 0.0, 10.5] -> prediction: 0.9127751255680273\n",
      "Sample: [3.0, 0.0, 27.0, 0.0, 0.0, 6.8583] -> prediction: 0.11224617800350173\n",
      "Sample: [3.0, 0.0, 22.0, 0.0, 0.0, 7.8958] -> prediction: 0.13388922930541008\n",
      "Sample: [2.0, 0.0, 44.0, 1.0, 0.0, 26.0] -> prediction: 0.13033500238760562\n",
      "Sample: [3.0, 1.0, 45.0, 0.0, 1.0, 14.4542] -> prediction: 0.5715064294675559\n",
      "Sample: [3.0, 1.0, 31.0, 0.0, 0.0, 7.7875] -> prediction: 0.7062001618865256\n",
      "Sample: [3.0, 1.0, 9.0, 4.0, 2.0, 31.275] -> prediction: 0.37145030673073937\n",
      "Sample: [1.0, 1.0, 24.0, 0.0, 0.0, 69.3] -> prediction: 0.9854856964849364\n",
      "Sample: [3.0, 0.0, 20.0, 0.0, 0.0, 7.8958] -> prediction: 0.14270530841918497\n",
      "Sample: [3.0, 0.0, 25.0, 1.0, 0.0, 7.775] -> prediction: 0.0731689597464982\n",
      "Sample: [1.0, 0.0, 29.0, 0.0, 0.0, 30.0] -> prediction: 0.5837046173683925\n",
      "Sample: [3.0, 1.0, 11.0, 4.0, 2.0, 31.275] -> prediction: 0.35434369377420455\n",
      "Sample: [3.0, 1.0, 31.0, 0.0, 0.0, 7.8542] -> prediction: 0.7064076005885831\n",
      "Sample: [1.0, 0.0, 60.0, 1.0, 1.0, 79.2] -> prediction: 0.31583050360187015\n",
      "Sample: [2.0, 1.0, 7.0, 0.0, 2.0, 26.25] -> prediction: 0.9407549244177343\n",
      "Sample: [3.0, 0.0, 15.0, 1.0, 1.0, 7.2292] -> prediction: 0.08698631993184285\n",
      "Sample: [1.0, 1.0, 36.0, 1.0, 2.0, 120.0] -> prediction: 0.9749130378465203\n",
      "Sample: [1.0, 1.0, 35.0, 1.0, 0.0, 52.0] -> prediction: 0.9516623712800949\n",
      "Sample: [3.0, 0.0, 29.0, 0.0, 0.0, 9.4833] -> prediction: 0.10899966447777855\n",
      "Sample: [2.0, 0.0, 31.0, 0.0, 0.0, 13.0] -> prediction: 0.2571183997812366\n",
      "Sample: [3.0, 0.0, 19.0, 0.0, 0.0, 14.5] -> prediction: 0.1608388270389613\n",
      "Sample: [2.0, 0.0, 30.0, 1.0, 0.0, 24.0] -> prediction: 0.19607637802474104\n",
      "Sample: [3.0, 0.0, 16.0, 0.0, 0.0, 9.5] -> prediction: 0.16534282858797356\n",
      "Sample: [3.0, 0.0, 35.0, 0.0, 0.0, 7.05] -> prediction: 0.08611666298241182\n",
      "Sample: [1.0, 1.0, 36.0, 0.0, 0.0, 135.6333] -> prediction: 0.9919420059862908\n",
      "Sample: [1.0, 0.0, 56.0, 0.0, 0.0, 26.55] -> prediction: 0.328274417594861\n",
      "Sample: [1.0, 0.0, 29.0, 1.0, 0.0, 66.6] -> prediction: 0.5873448815653372\n",
      "Sample: [3.0, 0.0, 2.0, 3.0, 1.0, 21.075] -> prediction: 0.058911548026713854\n",
      "Sample: [3.0, 0.0, 17.0, 1.0, 0.0, 7.0542] -> prediction: 0.0950049911465906\n",
      "Sample: [3.0, 1.0, 16.0, 0.0, 0.0, 7.7333] -> prediction: 0.8070570795320793\n",
      "Sample: [2.0, 1.0, 40.0, 0.0, 0.0, 13.0] -> prediction: 0.8434334708361501\n",
      "Sample: [3.0, 0.0, 30.0, 0.0, 0.0, 7.2292] -> prediction: 0.10220143584673717\n",
      "Sample: [1.0, 0.0, 21.0, 0.0, 1.0, 77.2875] -> prediction: 0.7687026301686054\n",
      "Sample: [3.0, 0.0, 28.0, 0.0, 0.0, 9.5] -> prediction: 0.1126453845407773\n",
      "Sample: [3.0, 0.0, 32.0, 0.0, 0.0, 56.4958] -> prediction: 0.18618382790150853\n",
      "Sample: [3.0, 0.0, 23.5, 0.0, 0.0, 7.2292] -> prediction: 0.12641889716063934\n",
      "Sample: [3.0, 1.0, 43.0, 1.0, 6.0, 46.9] -> prediction: 0.36308482471958553\n",
      "Sample: [3.0, 0.0, 34.5, 0.0, 0.0, 6.4375] -> prediction: 0.08682761168194916\n",
      "Sample: [3.0, 0.0, 54.0, 0.0, 0.0, 7.25] -> prediction: 0.04470221737713016\n",
      "Sample: [3.0, 0.0, 19.0, 0.0, 0.0, 8.05] -> prediction: 0.14766920974173398\n",
      "Sample: [2.0, 0.0, 43.0, 1.0, 1.0, 26.25] -> prediction: 0.11588385502388747\n",
      "Sample: [3.0, 1.0, 26.0, 0.0, 0.0, 7.925] -> prediction: 0.7434542081463438\n",
      "Sample: [3.0, 0.0, 42.0, 0.0, 0.0, 7.8958] -> prediction: 0.06862532527007503\n",
      "Sample: [3.0, 0.0, 17.0, 0.0, 0.0, 8.6625] -> prediction: 0.15855755034872468\n",
      "Sample: [3.0, 0.0, 39.0, 0.0, 0.0, 7.925] -> prediction: 0.07613907106345413\n",
      "Sample: [3.0, 0.0, 3.0, 4.0, 2.0, 31.3875] -> prediction: 0.032926394798136256\n",
      "Sample: [3.0, 0.0, 5.0, 8.0, 2.0, 69.55] -> prediction: 0.006108177032957308\n",
      "Sample: [3.0, 0.0, 32.0, 0.0, 0.0, 8.3625] -> prediction: 0.09708864098649891\n",
      "Sample: [3.0, 1.0, 15.0, 1.0, 0.0, 14.4542] -> prediction: 0.7339975165215696\n",
      "Sample: [3.0, 1.0, 18.0, 0.0, 1.0, 9.35] -> prediction: 0.769944880799786\n",
      "Sample: [3.0, 1.0, 21.0, 0.0, 0.0, 7.6292] -> prediction: 0.7764331373333982\n",
      "Sample: [3.0, 0.0, 23.0, 0.0, 0.0, 7.225] -> prediction: 0.12853213950074904\n",
      "Sample: [3.0, 1.0, 20.0, 0.0, 0.0, 7.75] -> prediction: 0.7831298956134283\n",
      "Sample: [3.0, 1.0, 33.0, 1.0, 0.0, 16.1] -> prediction: 0.5924251646999523\n",
      "Sample: [3.0, 0.0, 10.0, 3.0, 2.0, 27.9] -> prediction: 0.041686405454024196\n",
      "Sample: [3.0, 0.0, 16.0, 0.0, 0.0, 8.7125] -> prediction: 0.1636934172971963\n",
      "Sample: [1.0, 0.0, 25.0, 1.0, 0.0, 91.0792] -> prediction: 0.7078572714244814\n",
      "Sample: [3.0, 0.0, 25.0, 1.0, 0.0, 17.8] -> prediction: 0.08455562510122677\n",
      "Sample: [1.0, 0.0, 35.0, 0.0, 0.0, 512.3292] -> prediction: 0.9995373144039121\n",
      "Sample: [3.0, 1.0, 18.0, 0.0, 0.0, 6.75] -> prediction: 0.7928186483122006\n",
      "Sample: [2.0, 0.0, 30.0, 0.0, 0.0, 13.0] -> prediction: 0.2642489816897696\n",
      "Sample: [3.0, 1.0, 5.0, 2.0, 1.0, 19.2583] -> prediction: 0.6741465983070782\n",
      "Sample: [3.0, 0.0, 22.0, 0.0, 0.0, 9.0] -> prediction: 0.13599035122043557\n",
      "Sample: [2.0, 0.0, 42.0, 0.0, 0.0, 13.0] -> prediction: 0.18709465662650418\n",
      "Sample: [2.0, 1.0, 19.0, 0.0, 0.0, 26.0] -> prediction: 0.9349295104719748\n",
      "Sample: [1.0, 1.0, 38.0, 0.0, 0.0, 80.0] -> prediction: 0.9795076820818311\n",
      "Sample: [3.0, 0.0, 20.0, 0.0, 0.0, 7.75] -> prediction: 0.14246080221548799\n",
      "Sample: [2.0, 0.0, 35.0, 0.0, 0.0, 10.5] -> prediction: 0.22287329161311592\n",
      "Sample: [1.0, 1.0, 58.0, 0.0, 1.0, 153.4625] -> prediction: 0.9837455410650685\n",
      "Sample: [3.0, 0.0, 33.0, 0.0, 0.0, 7.775] -> prediction: 0.09313023172022387\n",
      "Sample: [3.0, 0.0, 30.0, 0.0, 0.0, 8.05] -> prediction: 0.10330777942156368\n",
      "Sample: [2.0, 0.0, 25.0, 0.0, 0.0, 13.0] -> prediction: 0.30174543734530607\n",
      "Sample: [1.0, 1.0, 52.0, 1.0, 0.0, 78.2667] -> prediction: 0.9405874978712752\n",
      "Sample: [1.0, 0.0, 50.0, 1.0, 0.0, 106.425] -> prediction: 0.5495864684250396\n",
      "Sample: [2.0, 0.0, 36.0, 0.0, 0.0, 13.0] -> prediction: 0.22321988517324703\n",
      "Sample: [1.0, 0.0, 49.0, 1.0, 0.0, 89.1042] -> prediction: 0.4912508931197597\n",
      "Sample: [1.0, 0.0, 30.0, 0.0, 0.0, 0.0] -> prediction: 0.4578502721432993\n",
      "Sample: [3.0, 0.0, 23.0, 0.0, 0.0, 56.4958] -> prediction: 0.24195334785497452\n",
      "Sample: [1.0, 0.0, 36.0, 1.0, 2.0, 120.0] -> prediction: 0.6412974738474874\n",
      "Sample: [3.0, 0.0, 32.0, 0.0, 0.0, 8.05] -> prediction: 0.09665121094159101\n",
      "Sample: [1.0, 0.0, 25.0, 1.0, 0.0, 55.4417] -> prediction: 0.5807857981259906\n",
      "Sample: [2.0, 0.0, 3.0, 1.0, 1.0, 18.75] -> prediction: 0.33916891610855165\n",
      "Sample: [3.0, 0.0, 21.0, 0.0, 0.0, 7.925] -> prediction: 0.13835744472727332\n"
     ]
    }
   ],
   "source": [
    "# Now let's do some predictions\n",
    "\n",
    "y_pred_proba = [] # probability estimates\n",
    "\n",
    "for sample in X_test2:\n",
    "    mod_pred = predict(sample, b0, weights)\n",
    "    y_pred_proba.append(mod_pred)\n",
    "    \n",
    "    print(\"Sample: {} -> prediction: {}\".format(sample, mod_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3cace41",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [1 if prob >= 0.5 else 0 for prob in y_pred_proba]\n",
    "\n",
    "# [f(x) if condition else g(x) for x in sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39dda79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom implementation intercept & coefs: \n",
      "2.0010337665772653 [-1.05876709  3.07831146 -0.0370515  -0.55905229 -0.17472939  0.01567855]\n",
      "\n",
      "sklearn intercept & coefs: \n",
      "2.3453019357228344 [-1.12536695e+00  2.78094488e+00 -4.11665663e-02 -4.85412832e-01\n",
      " -1.04466608e-01  2.26963872e-03]\n"
     ]
    }
   ],
   "source": [
    "# Compare it to sklearn\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(fit_intercept=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nCustom implementation intercept & coefs: \")\n",
    "print(b0, weights)\n",
    "\n",
    "print(\"\\nsklearn intercept & coefs: \")\n",
    "print(clf.intercept_[0], clf.coef_[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fa57a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custom_weights</th>\n",
       "      <th>sklearn_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.058767</td>\n",
       "      <td>-1.125367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.078311</td>\n",
       "      <td>2.780945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.037052</td>\n",
       "      <td>-0.041167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.559052</td>\n",
       "      <td>-0.485413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.174729</td>\n",
       "      <td>-0.104467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.015679</td>\n",
       "      <td>0.002270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   custom_weights  sklearn_weights\n",
       "0       -1.058767        -1.125367\n",
       "1        3.078311         2.780945\n",
       "2       -0.037052        -0.041167\n",
       "3       -0.559052        -0.485413\n",
       "4       -0.174729        -0.104467\n",
       "5        0.015679         0.002270"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\"custom_weights\": weights, \"sklearn_weights\": clf.coef_[0]}\n",
    "\n",
    "df_weights = pd.DataFrame(d)\n",
    "df_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1e19295",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sk = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84592c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom implementation Acc. and MCC:\n",
      "Acc. : 0.7715355805243446\n",
      "MCC. : 0.5084157444744747\n",
      "\n",
      "sklearn implementation Acc. and MCC:\n",
      "Acc. sklearn : 0.7715355805243446\n",
      "MCC. sklearn : 0.5006253340310605\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"Custom implementation Acc. and MCC:\")\n",
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "mcc = metrics.matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "print(\"Acc. : {}\".format(acc))\n",
    "print(\"MCC. : {}\".format(mcc))\n",
    "\n",
    "\n",
    "acc = metrics.accuracy_score(y_test, y_pred_sk)\n",
    "mcc = metrics.matthews_corrcoef(y_test, y_pred_sk)\n",
    "\n",
    "print(\"\\nsklearn implementation Acc. and MCC:\")\n",
    "print(\"Acc. sklearn : {}\".format(acc))\n",
    "print(\"MCC. sklearn : {}\".format(mcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f7785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbbec5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
